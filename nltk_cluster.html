<!doctype html PUBLIC "-//W3C//DTD html 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
    <meta http-equiv='Content-Type' content='text/html; charset=utf-8'>
    
    
    <meta http-equiv='X-UA-Compatible' content='IE=emulateIE7' />
    <title>Coverage for nltk.cluster: 100%</title>
    <link rel='stylesheet' href='style.css' type='text/css'>
    
    <script type='text/javascript' src='jquery-1.4.3.min.js'></script>
    <script type='text/javascript' src='jquery.hotkeys.js'></script>
    <script type='text/javascript' src='jquery.isonscreen.js'></script>
    <script type='text/javascript' src='coverage_html.js'></script>
    <script type='text/javascript' charset='utf-8'>
        jQuery(document).ready(coverage.pyfile_ready);
    </script>
</head>
<body id='pyfile'>

<div id='header'>
    <div class='content'>
        <h1>Coverage for <b>nltk.cluster</b> :
            <span class='pc_cov'>100%</span>
        </h1>
        <img id='keyboard_icon' src='keybd_closed.png'>
        <h2 class='stats'>
            5 statements &nbsp;
            <span class='run hide_run shortkey_r button_toggle_run'>5 run</span>
            <span class='mis shortkey_m button_toggle_mis'>0 missing</span>
            <span class='exc shortkey_x button_toggle_exc'>0 excluded</span>
            
        </h2>
    </div>
</div>

<div class='help_panel'>
    <img id='panel_icon' src='keybd_open.png'>
<p class='legend'>Hot-keys on this page</p>
    <div>
<p class='keyhelp'>
        <span class='key'>r</span>
        <span class='key'>m</span>
        <span class='key'>x</span>
        <span class='key'>p</span> &nbsp; toggle line displays
    </p>
<p class='keyhelp'>
        <span class='key'>j</span>
        <span class='key'>k</span> &nbsp; next/prev highlighted chunk
    </p>
<p class='keyhelp'>
        <span class='key'>0</span> &nbsp; (zero) top of page
    </p>
<p class='keyhelp'>
        <span class='key'>1</span> &nbsp; (one) first highlighted chunk
    </p>
    </div>
</div>

<div id='source'>
    <table cellspacing='0' cellpadding='0'>
        <tr>
            <td class='linenos' valign='top'>
<p id='n1' class='pln'><a href='#n1'>1</a></p>
<p id='n2' class='pln'><a href='#n2'>2</a></p>
<p id='n3' class='pln'><a href='#n3'>3</a></p>
<p id='n4' class='pln'><a href='#n4'>4</a></p>
<p id='n5' class='pln'><a href='#n5'>5</a></p>
<p id='n6' class='pln'><a href='#n6'>6</a></p>
<p id='n7' class='pln'><a href='#n7'>7</a></p>
<p id='n8' class='stm run hide_run'><a href='#n8'>8</a></p>
<p id='n9' class='pln'><a href='#n9'>9</a></p>
<p id='n10' class='pln'><a href='#n10'>10</a></p>
<p id='n11' class='pln'><a href='#n11'>11</a></p>
<p id='n12' class='pln'><a href='#n12'>12</a></p>
<p id='n13' class='pln'><a href='#n13'>13</a></p>
<p id='n14' class='pln'><a href='#n14'>14</a></p>
<p id='n15' class='pln'><a href='#n15'>15</a></p>
<p id='n16' class='pln'><a href='#n16'>16</a></p>
<p id='n17' class='pln'><a href='#n17'>17</a></p>
<p id='n18' class='pln'><a href='#n18'>18</a></p>
<p id='n19' class='pln'><a href='#n19'>19</a></p>
<p id='n20' class='pln'><a href='#n20'>20</a></p>
<p id='n21' class='pln'><a href='#n21'>21</a></p>
<p id='n22' class='pln'><a href='#n22'>22</a></p>
<p id='n23' class='pln'><a href='#n23'>23</a></p>
<p id='n24' class='pln'><a href='#n24'>24</a></p>
<p id='n25' class='pln'><a href='#n25'>25</a></p>
<p id='n26' class='pln'><a href='#n26'>26</a></p>
<p id='n27' class='pln'><a href='#n27'>27</a></p>
<p id='n28' class='pln'><a href='#n28'>28</a></p>
<p id='n29' class='pln'><a href='#n29'>29</a></p>
<p id='n30' class='pln'><a href='#n30'>30</a></p>
<p id='n31' class='pln'><a href='#n31'>31</a></p>
<p id='n32' class='pln'><a href='#n32'>32</a></p>
<p id='n33' class='pln'><a href='#n33'>33</a></p>
<p id='n34' class='pln'><a href='#n34'>34</a></p>
<p id='n35' class='pln'><a href='#n35'>35</a></p>
<p id='n36' class='pln'><a href='#n36'>36</a></p>
<p id='n37' class='pln'><a href='#n37'>37</a></p>
<p id='n38' class='pln'><a href='#n38'>38</a></p>
<p id='n39' class='pln'><a href='#n39'>39</a></p>
<p id='n40' class='pln'><a href='#n40'>40</a></p>
<p id='n41' class='pln'><a href='#n41'>41</a></p>
<p id='n42' class='pln'><a href='#n42'>42</a></p>
<p id='n43' class='pln'><a href='#n43'>43</a></p>
<p id='n44' class='pln'><a href='#n44'>44</a></p>
<p id='n45' class='pln'><a href='#n45'>45</a></p>
<p id='n46' class='pln'><a href='#n46'>46</a></p>
<p id='n47' class='pln'><a href='#n47'>47</a></p>
<p id='n48' class='pln'><a href='#n48'>48</a></p>
<p id='n49' class='pln'><a href='#n49'>49</a></p>
<p id='n50' class='pln'><a href='#n50'>50</a></p>
<p id='n51' class='pln'><a href='#n51'>51</a></p>
<p id='n52' class='pln'><a href='#n52'>52</a></p>
<p id='n53' class='pln'><a href='#n53'>53</a></p>
<p id='n54' class='pln'><a href='#n54'>54</a></p>
<p id='n55' class='pln'><a href='#n55'>55</a></p>
<p id='n56' class='pln'><a href='#n56'>56</a></p>
<p id='n57' class='pln'><a href='#n57'>57</a></p>
<p id='n58' class='pln'><a href='#n58'>58</a></p>
<p id='n59' class='pln'><a href='#n59'>59</a></p>
<p id='n60' class='pln'><a href='#n60'>60</a></p>
<p id='n61' class='pln'><a href='#n61'>61</a></p>
<p id='n62' class='pln'><a href='#n62'>62</a></p>
<p id='n63' class='pln'><a href='#n63'>63</a></p>
<p id='n64' class='pln'><a href='#n64'>64</a></p>
<p id='n65' class='pln'><a href='#n65'>65</a></p>
<p id='n66' class='pln'><a href='#n66'>66</a></p>
<p id='n67' class='pln'><a href='#n67'>67</a></p>
<p id='n68' class='pln'><a href='#n68'>68</a></p>
<p id='n69' class='pln'><a href='#n69'>69</a></p>
<p id='n70' class='pln'><a href='#n70'>70</a></p>
<p id='n71' class='pln'><a href='#n71'>71</a></p>
<p id='n72' class='pln'><a href='#n72'>72</a></p>
<p id='n73' class='pln'><a href='#n73'>73</a></p>
<p id='n74' class='pln'><a href='#n74'>74</a></p>
<p id='n75' class='pln'><a href='#n75'>75</a></p>
<p id='n76' class='pln'><a href='#n76'>76</a></p>
<p id='n77' class='pln'><a href='#n77'>77</a></p>
<p id='n78' class='pln'><a href='#n78'>78</a></p>
<p id='n79' class='pln'><a href='#n79'>79</a></p>
<p id='n80' class='pln'><a href='#n80'>80</a></p>
<p id='n81' class='pln'><a href='#n81'>81</a></p>
<p id='n82' class='stm run hide_run'><a href='#n82'>82</a></p>
<p id='n83' class='pln'><a href='#n83'>83</a></p>
<p id='n84' class='stm run hide_run'><a href='#n84'>84</a></p>
<p id='n85' class='stm run hide_run'><a href='#n85'>85</a></p>
<p id='n86' class='stm run hide_run'><a href='#n86'>86</a></p>
                
            </td>
            <td class='text' valign='top'>
<p id='t1' class='pln'><span class='com'># Natural Language Toolkit: Clusterers</span><span class='strut'>&nbsp;</span></p>
<p id='t2' class='pln'><span class='com'>#</span><span class='strut'>&nbsp;</span></p>
<p id='t3' class='pln'><span class='com'># Copyright (C) 2001-2012 NLTK Project</span><span class='strut'>&nbsp;</span></p>
<p id='t4' class='pln'><span class='com'># Author: Trevor Cohn &lt;tacohn@cs.mu.oz.au&gt;</span><span class='strut'>&nbsp;</span></p>
<p id='t5' class='pln'><span class='com'># URL: &lt;http://www.nltk.org/&gt;</span><span class='strut'>&nbsp;</span></p>
<p id='t6' class='pln'><span class='com'># For license information, see LICENSE.TXT</span><span class='strut'>&nbsp;</span></p>
<p id='t7' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t8' class='stm run hide_run'><span class='str'>&quot;&quot;&quot;</span><span class='strut'>&nbsp;</span></p>
<p id='t9' class='pln'><span class='str'>This module contains a number of basic clustering algorithms. Clustering</span><span class='strut'>&nbsp;</span></p>
<p id='t10' class='pln'><span class='str'>describes the task of discovering groups of similar items with a large</span><span class='strut'>&nbsp;</span></p>
<p id='t11' class='pln'><span class='str'>collection. It is also describe as unsupervised machine learning, as the data</span><span class='strut'>&nbsp;</span></p>
<p id='t12' class='pln'><span class='str'>from which it learns is unannotated with class information, as is the case for</span><span class='strut'>&nbsp;</span></p>
<p id='t13' class='pln'><span class='str'>supervised learning.&nbsp; Annotated data is difficult and expensive to obtain in</span><span class='strut'>&nbsp;</span></p>
<p id='t14' class='pln'><span class='str'>the quantities required for the majority of supervised learning algorithms.</span><span class='strut'>&nbsp;</span></p>
<p id='t15' class='pln'><span class='str'>This problem, the knowledge acquisition bottleneck, is common to most natural</span><span class='strut'>&nbsp;</span></p>
<p id='t16' class='pln'><span class='str'>language processing tasks, thus fueling the need for quality unsupervised</span><span class='strut'>&nbsp;</span></p>
<p id='t17' class='pln'><span class='str'>approaches.</span><span class='strut'>&nbsp;</span></p>
<p id='t18' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t19' class='pln'><span class='str'>This module contains a k-means clusterer, E-M clusterer and a group average</span><span class='strut'>&nbsp;</span></p>
<p id='t20' class='pln'><span class='str'>agglomerative clusterer (GAAC). All these clusterers involve finding good</span><span class='strut'>&nbsp;</span></p>
<p id='t21' class='pln'><span class='str'>cluster groupings for a set of vectors in multi-dimensional space.</span><span class='strut'>&nbsp;</span></p>
<p id='t22' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t23' class='pln'><span class='str'>The K-means clusterer starts with k arbitrary chosen means then allocates each</span><span class='strut'>&nbsp;</span></p>
<p id='t24' class='pln'><span class='str'>vector to the cluster with the closest mean. It then recalculates the means of</span><span class='strut'>&nbsp;</span></p>
<p id='t25' class='pln'><span class='str'>each cluster as the centroid of the vectors in the cluster. This process</span><span class='strut'>&nbsp;</span></p>
<p id='t26' class='pln'><span class='str'>repeats until the cluster memberships stabilise. This is a hill-climbing</span><span class='strut'>&nbsp;</span></p>
<p id='t27' class='pln'><span class='str'>algorithm which may converge to a local maximum. Hence the clustering is</span><span class='strut'>&nbsp;</span></p>
<p id='t28' class='pln'><span class='str'>often repeated with random initial means and the most commonly occurring</span><span class='strut'>&nbsp;</span></p>
<p id='t29' class='pln'><span class='str'>output means are chosen.</span><span class='strut'>&nbsp;</span></p>
<p id='t30' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t31' class='pln'><span class='str'>The GAAC clusterer starts with each of the *N* vectors as singleton clusters.</span><span class='strut'>&nbsp;</span></p>
<p id='t32' class='pln'><span class='str'>It then iteratively merges pairs of clusters which have the closest centroids.</span><span class='strut'>&nbsp;</span></p>
<p id='t33' class='pln'><span class='str'>This continues until there is only one cluster. The order of merges gives rise</span><span class='strut'>&nbsp;</span></p>
<p id='t34' class='pln'><span class='str'>to a dendrogram - a tree with the earlier merges lower than later merges. The</span><span class='strut'>&nbsp;</span></p>
<p id='t35' class='pln'><span class='str'>membership of a given number of clusters *c*, *1 &lt;= c &lt;= N*, can be found by</span><span class='strut'>&nbsp;</span></p>
<p id='t36' class='pln'><span class='str'>cutting the dendrogram at depth *c*.</span><span class='strut'>&nbsp;</span></p>
<p id='t37' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t38' class='pln'><span class='str'>The Gaussian EM clusterer models the vectors as being produced by a mixture</span><span class='strut'>&nbsp;</span></p>
<p id='t39' class='pln'><span class='str'>of k Gaussian sources. The parameters of these sources (prior probability,</span><span class='strut'>&nbsp;</span></p>
<p id='t40' class='pln'><span class='str'>mean and covariance matrix) are then found to maximise the likelihood of the</span><span class='strut'>&nbsp;</span></p>
<p id='t41' class='pln'><span class='str'>given data. This is done with the expectation maximisation algorithm. It</span><span class='strut'>&nbsp;</span></p>
<p id='t42' class='pln'><span class='str'>starts with k arbitrarily chosen means, priors and covariance matrices. It</span><span class='strut'>&nbsp;</span></p>
<p id='t43' class='pln'><span class='str'>then calculates the membership probabilities for each vector in each of the</span><span class='strut'>&nbsp;</span></p>
<p id='t44' class='pln'><span class='str'>clusters - this is the &#39;E&#39; step. The cluster parameters are then updated in</span><span class='strut'>&nbsp;</span></p>
<p id='t45' class='pln'><span class='str'>the &#39;M&#39; step using the maximum likelihood estimate from the cluster membership</span><span class='strut'>&nbsp;</span></p>
<p id='t46' class='pln'><span class='str'>probabilities. This process continues until the likelihood of the data does</span><span class='strut'>&nbsp;</span></p>
<p id='t47' class='pln'><span class='str'>not significantly increase.</span><span class='strut'>&nbsp;</span></p>
<p id='t48' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t49' class='pln'><span class='str'>They all extend the ClusterI interface which defines common operations</span><span class='strut'>&nbsp;</span></p>
<p id='t50' class='pln'><span class='str'>available with each clusterer. These operations include.</span><span class='strut'>&nbsp;</span></p>
<p id='t51' class='pln'><span class='str'>&nbsp;&nbsp; - cluster: clusters a sequence of vectors</span><span class='strut'>&nbsp;</span></p>
<p id='t52' class='pln'><span class='str'>&nbsp;&nbsp; - classify: assign a vector to a cluster</span><span class='strut'>&nbsp;</span></p>
<p id='t53' class='pln'><span class='str'>&nbsp;&nbsp; - classification_probdist: give the probability distribution over cluster memberships</span><span class='strut'>&nbsp;</span></p>
<p id='t54' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t55' class='pln'><span class='str'>The current existing classifiers also extend cluster.VectorSpace, an</span><span class='strut'>&nbsp;</span></p>
<p id='t56' class='pln'><span class='str'>abstract class which allows for singular value decomposition (SVD) and vector</span><span class='strut'>&nbsp;</span></p>
<p id='t57' class='pln'><span class='str'>normalisation. SVD is used to reduce the dimensionality of the vector space in</span><span class='strut'>&nbsp;</span></p>
<p id='t58' class='pln'><span class='str'>such a manner as to preserve as much of the variation as possible, by</span><span class='strut'>&nbsp;</span></p>
<p id='t59' class='pln'><span class='str'>reparameterising the axes in order of variability and discarding all bar the</span><span class='strut'>&nbsp;</span></p>
<p id='t60' class='pln'><span class='str'>first d dimensions. Normalisation ensures that vectors fall in the unit</span><span class='strut'>&nbsp;</span></p>
<p id='t61' class='pln'><span class='str'>hypersphere.</span><span class='strut'>&nbsp;</span></p>
<p id='t62' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t63' class='pln'><span class='str'>Usage example (see also demo())::</span><span class='strut'>&nbsp;</span></p>
<p id='t64' class='pln'><span class='str'>&nbsp; &nbsp; from nltk import cluster</span><span class='strut'>&nbsp;</span></p>
<p id='t65' class='pln'><span class='str'>&nbsp; &nbsp; from nltk.cluster import euclidean_distance</span><span class='strut'>&nbsp;</span></p>
<p id='t66' class='pln'><span class='str'>&nbsp; &nbsp; from numpy import array</span><span class='strut'>&nbsp;</span></p>
<p id='t67' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t68' class='pln'><span class='str'>&nbsp; &nbsp; vectors = [array(f) for f in [[3, 3], [1, 2], [4, 2], [4, 0]]]</span><span class='strut'>&nbsp;</span></p>
<p id='t69' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t70' class='pln'><span class='str'>&nbsp; &nbsp; # initialise the clusterer (will also assign the vectors to clusters)</span><span class='strut'>&nbsp;</span></p>
<p id='t71' class='pln'><span class='str'>&nbsp; &nbsp; clusterer = cluster.KMeansClusterer(2, euclidean_distance)</span><span class='strut'>&nbsp;</span></p>
<p id='t72' class='pln'><span class='str'>&nbsp; &nbsp; clusterer.cluster(vectors, True)</span><span class='strut'>&nbsp;</span></p>
<p id='t73' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t74' class='pln'><span class='str'>&nbsp; &nbsp; # classify a new vector</span><span class='strut'>&nbsp;</span></p>
<p id='t75' class='pln'><span class='str'>&nbsp; &nbsp; print clusterer.classify(array([3, 3]))</span><span class='strut'>&nbsp;</span></p>
<p id='t76' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t77' class='pln'><span class='str'>Note that the vectors must use numpy array-like</span><span class='strut'>&nbsp;</span></p>
<p id='t78' class='pln'><span class='str'>objects. nltk_contrib.unimelb.tacohn.SparseArrays may be used for</span><span class='strut'>&nbsp;</span></p>
<p id='t79' class='pln'><span class='str'>efficiency when required.</span><span class='strut'>&nbsp;</span></p>
<p id='t80' class='pln'><span class='str'>&quot;&quot;&quot;</span><span class='strut'>&nbsp;</span></p>
<p id='t81' class='pln'><span class='strut'>&nbsp;</span></p>
<p id='t82' class='stm run hide_run'><span class='key'>from</span> <span class='nam'>nltk</span><span class='op'>.</span><span class='nam'>cluster</span><span class='op'>.</span><span class='nam'>util</span> <span class='key'>import</span> <span class='op'>(</span><span class='nam'>VectorSpaceClusterer</span><span class='op'>,</span> <span class='nam'>Dendrogram</span><span class='op'>,</span><span class='strut'>&nbsp;</span></p>
<p id='t83' class='pln'>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <span class='nam'>euclidean_distance</span><span class='op'>,</span> <span class='nam'>cosine_distance</span><span class='op'>)</span><span class='strut'>&nbsp;</span></p>
<p id='t84' class='stm run hide_run'><span class='key'>from</span> <span class='nam'>nltk</span><span class='op'>.</span><span class='nam'>cluster</span><span class='op'>.</span><span class='nam'>kmeans</span> <span class='key'>import</span> <span class='nam'>KMeansClusterer</span><span class='strut'>&nbsp;</span></p>
<p id='t85' class='stm run hide_run'><span class='key'>from</span> <span class='nam'>nltk</span><span class='op'>.</span><span class='nam'>cluster</span><span class='op'>.</span><span class='nam'>gaac</span> <span class='key'>import</span> <span class='nam'>GAAClusterer</span><span class='strut'>&nbsp;</span></p>
<p id='t86' class='stm run hide_run'><span class='key'>from</span> <span class='nam'>nltk</span><span class='op'>.</span><span class='nam'>cluster</span><span class='op'>.</span><span class='nam'>em</span> <span class='key'>import</span> <span class='nam'>EMClusterer</span><span class='strut'>&nbsp;</span></p>
                
            </td>
        </tr>
    </table>
</div>

<div id='footer'>
    <div class='content'>
        <p>
            <a class='nav' href='index.html'>&#xab; index</a> &nbsp; &nbsp; <a class='nav' href='http://nedbatchelder.com/code/coverage'>coverage.py v3.5.2</a>
        </p>
    </div>
</div>

</body>
</html>
